{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1 for KDSH2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we have to categorize the research papers into publishable and non-publishable"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": 1,
=======
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
>>>>>>> 419cbddfd154b28877690b6ce9a8c98aedcb81a9
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "abcd\n"
     ]
    }
   ],
   "source": []
=======
      "Leveraging Clustering Techniques for Enhanced\n",
      "Drone Monitoring and Position Estimation\n",
      "Abstract\n",
      "Drone tracking and localization are essential for various applications, including\n",
      "managing drone formations and implementing anti-drone strategies. Pinpointing\n",
      "and monitoring drones in three-dimensional space is difficult, particularly when\n",
      "trying to capture the subtle movements of small drones during rapid maneuvers.\n",
      "This involves extracting faint signals from varied flight settings and maintaining\n",
      "alignment despite swift actions. Typically, cameras and LiDAR systems are used\n",
      "to record the paths of drones. However, they encounter challenges in categorizing\n",
      "drones and estimating their positions accurately. This report provides an overview\n",
      "of an approach named CL-Det. It uses a clustering-based learning detection strategy\n",
      "to track and estimate the position of drones using data from two types of LiDAR\n",
      "sensors: Livox Avia and LiDAR 360. This method merges data from both LiDAR\n",
      "sources to accurately determine the drone’s location in three dimensions. The\n",
      "method begins by synchronizing the time codes of the data from the two sensors\n",
      "and then isolates the point cloud data for the objects of interest (OOIs) from the\n",
      "environmental data. A Density-Based Spatial Clustering of Applications with\n",
      "Noise (DBSCAN) method is applied to cluster the OOI point cloud data, and the\n",
      "center point of the most prominent cluster is taken as the drone’s location. The\n",
      "technique also incorporates past position estimates to compensate for any missing\n",
      "information.\n",
      "1 Introduction\n",
      "Unmanned aerial vehicles (UA Vs), commonly referred to as drones, have gained prominence and\n",
      "significantly influence areas like logistics, imaging, and emergency response, offering substantial\n",
      "advantages to society. However, the broad adoption and sophisticated features of compact, off-the-\n",
      "shelf drones have created intricate security issues that extend beyond conventional risks.\n",
      "Recent years have witnessed a surge in research on anti-UA V systems. Present anti-UA V methods\n",
      "predominantly utilize visual, radar, and radio frequency (RF) technologies. Despite these strides,\n",
      "recognizing drones poses a considerable hurdle for sensors like cameras, particularly when drones\n",
      "are at significant altitudes or in challenging visual environments. These methods usually fail to spot\n",
      "small drones because of their minimal size, which leads to a decreased radar cross-section and a\n",
      "less noticeable visual presence. Furthermore, current anti-UA V studies primarily focus on detecting\n",
      "objects and tracking them in two dimensions, overlooking the crucial element of estimating their\n",
      "3D paths. This omission significantly restricts the effectiveness of anti-UA V systems in practical,\n",
      "real-world contexts.\n",
      "Our proposed solution, a detection method based on clustering learning (CL-Det), uses the strengths\n",
      "of both Livox Avia and LiDAR 360 to improve the tracking and position estimation of UA Vs.\n",
      "Initially, the timestamps from the Livox Avia and LiDAR 360 data are aligned to maintain temporal\n",
      "consistency. By examining the LiDAR data, which contains the spatial coordinates of objects at\n",
      "specific times, and comparing these to the actual recorded positions of the drone at those times, the\n",
      "drone’s location within the LiDAR point cloud data is effectively pinpointed. The point cloud for\n",
      ".objects of interest (OOIs) is then isolated from the environmental data. The point cloud of the OOIs\n",
      "is grouped using the DBSCAN algorithm, and the central point of the largest cluster is designated as\n",
      "the UA V’s position. Moreover, radar data also faces significant challenges due to missing information.\n",
      "To mitigate potential data deficiencies, past estimations are employed to supplement missing data,\n",
      "thereby maintaining the consistency and precision of UA V tracking.\n",
      "2 Methodology\n",
      "This section details the methodology employed to ascertain the drone’s spatial position utilizing\n",
      "information from LiDAR 360 and Livox Avia sensors. The strategy integrates data from both sensor\n",
      "types to achieve precise position calculations.\n",
      "2.1 Data Sources\n",
      "The following modalities of data were utilized:\n",
      "• Double fisheye camera visual images\n",
      "• Livox Mid-360 (LiDAR 360) 3D point cloud data\n",
      "• Livox Avia 3D point cloud data\n",
      "• Millimeter-wave radar 3D point cloud data\n",
      "Only 14 out of 59 test sequences have non-zero radar values; therefore, the radar dataset is excluded\n",
      "from this work due to data availability issues. Two primary sensor types are employed: LiDAR 360\n",
      "and Livox Avia, both of which supply 3D point cloud data crucial for identifying the drone’s location.\n",
      "The detailed data descriptions are outlined as follows:\n",
      "•LiDAR 360 offers a complete 360-degree view with 3D point cloud data. This dataset\n",
      "encompasses environmental details and other observable objects.\n",
      "•Livox Avia delivers focused 3D point cloud data at specific timestamps, typically indicating\n",
      "the origin point or the drone’s position.\n",
      "2.2 Algorithm\n",
      "For every sequence, corresponding positions are recorded at specific timestamps. The procedure\n",
      "gives precedence to LiDAR 360 data, using Livox Avia data as a backup if the former is not available.\n",
      "If neither source is accessible, the position is estimated using historical averages.\n",
      "2.2.1 LiDAR 360 Data Processing\n",
      "•Separation of Points: The LiDAR 360 data is visually examined to classify areas into two\n",
      "zones: environment and non-environment zones.\n",
      "•Removal of Environment Points: All points within the environment zone are deemed part\n",
      "of the surroundings and are thus excluded from the dataset. After removing environment\n",
      "points, it is observed that the remaining non-environment points imply the drone position.\n",
      "•Clustering: The DBSCAN clustering algorithm is applied to the remaining points to discern\n",
      "distinct clusters.\n",
      "•Cluster Selection: The most extensive non-environment cluster is chosen as the representa-\n",
      "tive group of points that correspond to the drone.\n",
      "•Mean Position Calculation: The drone’s position is determined by calculating the mean of\n",
      "the selected cluster, represented by (x, y, z) coordinates.\n",
      "2.2.2 Livox Avia Data Processing\n",
      "•Removal of Noise: Points with coordinates (0, 0, 0) are eliminated as they are regarded as\n",
      "noise.\n",
      "•Mean Position Calculation: The mean of the residual points is computed to ascertain the\n",
      "drone’s position in (x, y, z) coordinates.\n",
      "22.2.3 Fallback Method\n",
      "When neither LiDAR 360 nor Livox Avia data is available, the average location of the drone derived\n",
      "from training datasets is used. The average ground truth position (x, y, z) from all training datasets\n",
      "estimates the drone ground truth position, which is (0.734, -9.739, 33.353).\n",
      "2.3 Implementation Details\n",
      "The program fetches LiDAR 360 or Livox Avia data from the nearest timestamp for each sequence,\n",
      "as indicated in the test dataset. Clustering is executed using the DBSCAN algorithm with appro-\n",
      "priate parameters to guarantee strong clustering. Visual inspection is employed for the preliminary\n",
      "separation of points, ensuring an accurate categorization of environment points.\n",
      "The implementation was conducted on a Lenovo IdeaPad Slim 5 Pro (16\") running Windows 11\n",
      "with an AMD Ryzen 7 5800H CPU and 16GB DDR4 RAM. The analysis was carried out in a\n",
      "Jupyter Notebook environment using Python 3.10. For clustering, the DBSCAN algorithm from the\n",
      "Scikit-Learn library was utilized. The DBSCAN algorithm was configured with an epsilon (eps)\n",
      "value of 2 and a minimum number of points (minPts) set to 1.\n",
      "3 Results\n",
      "The algorithm achieved a pose MSE loss of 120.215 and a classification accuracy of 0.322. Table 1\n",
      "presents the evaluation results compared to other teams.\n",
      "Table 1: Evaluation results on the leaderboard\n",
      "Team ID Pose MSE ( ↓) Accuracy ( ↑)\n",
      "SDUCZS 58198 2.21375 0.8136\n",
      "Gaofen Lab 57978 7.299575 0.3220\n",
      "sysutlt 57843 24.50694 0.3220\n",
      "casetrous 58233 56.880267 0.2542\n",
      "NTU-ICG (ours) 58268 120.215107 0.3220\n",
      "MTC 58180 189.669428 0.2724\n",
      "gzist 56936 417.396317 0.2302\n",
      "4 Conclusions\n",
      "This paper introduces a clustering-based learning method, CL-Det, which employs advanced cluster-\n",
      "ing techniques such as K-Means and DBSCAN for drone detection and position estimation using\n",
      "LiDAR data. The approach guarantees dependable and precise drone position estimation by utilizing\n",
      "multi-sensor data and robust clustering methods. Fallback mechanisms are in place to ensure con-\n",
      "tinuous position estimation even when primary sensor data is absent. Through thorough parameter\n",
      "optimization and comparative assessment, the proposed method’s effective performance in drone\n",
      "tracking and position estimation is demonstrated.\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    with open(file_path, 'rb') as pdf_file:\n",
    "        reader = PyPDF2.PdfReader(pdf_file)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "        return text\n",
    "\n",
    "pdf_text = extract_text_from_pdf('Dataset/Papers/P001.pdf')\n",
    "print(pdf_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drone tracking and localization are essential for various applications, including\n",
      "managing drone formations and implementing anti-drone strategies. Pinpointing\n",
      "and monitoring drones in three-dimensional space is difficult, particularly when\n",
      "trying to capture the subtle movements of small drones during rapid maneuvers.\n",
      "This involves extracting faint signals from varied flight settings and maintaining\n",
      "alignment despite swift actions. Typically, cameras and LiDAR systems are used\n",
      "to record the paths of drones. However, they encounter challenges in categorizing\n",
      "drones and estimating their positions accurately. This report provides an overview\n",
      "of an approach named CL-Det. It uses a clustering-based learning detection strategy\n",
      "to track and estimate the position of drones using data from two types of LiDAR\n",
      "sensors: Livox Avia and LiDAR 360. This method merges data from both LiDAR\n",
      "sources to accurately determine the drone’s location in three dimensions. The\n",
      "method begins by synchronizing the time codes of the data from the two sensors\n",
      "and then isolates the point cloud data for the objects of interest (OOIs) from the\n",
      "environmental data. A Density-Based Spatial Clustering of Applications with\n",
      "Noise (DBSCAN) method is applied to cluster the OOI point cloud data, and the\n",
      "center point of the most prominent cluster is taken as the drone’s location. The\n",
      "technique also incorporates past position estimates to compensate for any missing\n",
      "information.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def split_into_sections(text):\n",
    "    sections = re.split(r'\\b(Abstract|Introduction|Methods|Results|Discussion|Conclusion)\\b', text, flags=re.IGNORECASE)\n",
    "    return {sections[i].strip(): sections[i + 1].strip() for i in range(1, len(sections) - 1, 2)}\n",
    "\n",
    "sections = split_into_sections(pdf_text)\n",
    "print(sections['Abstract'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Drone', 'tracking', 'and', 'localization', 'are', 'essential', 'for', 'various', 'applications', ',', 'including', '\\n', 'managing', 'drone', 'formations', 'and', 'implementing', 'anti', '-', 'drone', 'strategies', '.', 'Pinpointing', '\\n', 'and', 'monitoring', 'drones', 'in', 'three', '-', 'dimensional', 'space', 'is', 'difficult', ',', 'particularly', 'when', '\\n', 'trying', 'to', 'capture', 'the', 'subtle', 'movements', 'of', 'small', 'drones', 'during', 'rapid', 'maneuvers', '.', '\\n', 'This', 'involves', 'extracting', 'faint', 'signals', 'from', 'varied', 'flight', 'settings', 'and', 'maintaining', '\\n', 'alignment', 'despite', 'swift', 'actions', '.', 'Typically', ',', 'cameras', 'and', 'LiDAR', 'systems', 'are', 'used', '\\n', 'to', 'record', 'the', 'paths', 'of', 'drones', '.', 'However', ',', 'they', 'encounter', 'challenges', 'in', 'categorizing', '\\n', 'drones', 'and', 'estimating', 'their', 'positions', 'accurately', '.', 'This', 'report', 'provides', 'an', 'overview', '\\n', 'of', 'an', 'approach', 'named', 'CL', '-', 'Det', '.', 'It', 'uses', 'a', 'clustering', '-', 'based', 'learning', 'detection', 'strategy', '\\n', 'to', 'track', 'and', 'estimate', 'the', 'position', 'of', 'drones', 'using', 'data', 'from', 'two', 'types', 'of', 'LiDAR', '\\n', 'sensors', ':', 'Livox', 'Avia', 'and', 'LiDAR', '360', '.', 'This', 'method', 'merges', 'data', 'from', 'both', 'LiDAR', '\\n', 'sources', 'to', 'accurately', 'determine', 'the', 'drone', '’s', 'location', 'in', 'three', 'dimensions', '.', 'The', '\\n', 'method', 'begins', 'by', 'synchronizing', 'the', 'time', 'codes', 'of', 'the', 'data', 'from', 'the', 'two', 'sensors', '\\n', 'and', 'then', 'isolates', 'the', 'point', 'cloud', 'data', 'for', 'the', 'objects', 'of', 'interest', '(', 'OOIs', ')', 'from', 'the', '\\n', 'environmental', 'data', '.', 'A', 'Density', '-', 'Based', 'Spatial', 'Clustering', 'of', 'Applications', 'with', '\\n', 'Noise', '(', 'DBSCAN', ')', 'method', 'is', 'applied', 'to', 'cluster', 'the', 'OOI', 'point', 'cloud', 'data', ',', 'and', 'the', '\\n', 'center', 'point', 'of', 'the', 'most', 'prominent', 'cluster', 'is', 'taken', 'as', 'the', 'drone', '’s', 'location', '.', 'The', '\\n', 'technique', 'also', 'incorporates', 'past', 'position', 'estimates', 'to', 'compensate', 'for', 'any', 'missing', '\\n', 'information', '.', '\\n', '1']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(sections['Abstract'])\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Drone', 'tracking', 'localization', 'essential', 'various', 'applications', ',', 'including', 'managing', 'drone', 'formations', 'implementing', 'anti-drone', 'strategies', '.', 'Pinpointing', 'monitoring', 'drones', 'three-dimensional', 'space', 'difficult', ',', 'particularly', 'trying', 'capture', 'subtle', 'movements', 'small', 'drones', 'rapid', 'maneuvers', '.', 'involves', 'extracting', 'faint', 'signals', 'varied', 'flight', 'settings', 'maintaining', 'alignment', 'despite', 'swift', 'actions', '.', 'Typically', ',', 'cameras', 'LiDAR', 'systems', 'used', 'record', 'paths', 'drones', '.', 'However', ',', 'encounter', 'challenges', 'categorizing', 'drones', 'estimating', 'positions', 'accurately', '.', 'report', 'provides', 'overview', 'approach', 'named', 'CL-Det', '.', 'uses', 'clustering-based', 'learning', 'detection', 'strategy', 'track', 'estimate', 'position', 'drones', 'using', 'data', 'two', 'types', 'LiDAR', 'sensors', ':', 'Livox', 'Avia', 'LiDAR', '360', '.', 'method', 'merges', 'data', 'LiDAR', 'sources', 'accurately', 'determine', 'drone', '’', 'location', 'three', 'dimensions', '.', 'method', 'begins', 'synchronizing', 'time', 'codes', 'data', 'two', 'sensors', 'isolates', 'point', 'cloud', 'data', 'objects', 'interest', '(', 'OOIs', ')', 'environmental', 'data', '.', 'Density-Based', 'Spatial', 'Clustering', 'Applications', 'Noise', '(', 'DBSCAN', ')', 'method', 'applied', 'cluster', 'OOI', 'point', 'cloud', 'data', ',', 'center', 'point', 'prominent', 'cluster', 'taken', 'drone', '’', 'location', '.', 'technique', 'also', 'incorporates', 'past', 'position', 'estimates', 'compensate', 'missing', 'information', '.', '1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\jbsch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = word_tokenize(sections['Abstract'])\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['drone', 'tracking', 'localization', 'essential', 'various', 'applications', 'including', 'managing', 'drone', 'formations', 'implementing', 'strategies', 'pinpointing', 'monitoring', 'drones', 'space', 'difficult', 'particularly', 'trying', 'capture', 'subtle', 'movements', 'small', 'drones', 'rapid', 'maneuvers', 'involves', 'extracting', 'faint', 'signals', 'varied', 'flight', 'settings', 'maintaining', 'alignment', 'despite', 'swift', 'actions', 'typically', 'cameras', 'lidar', 'systems', 'used', 'record', 'paths', 'drones', 'however', 'encounter', 'challenges', 'categorizing', 'drones', 'estimating', 'positions', 'accurately', 'report', 'provides', 'overview', 'approach', 'named', 'uses', 'learning', 'detection', 'strategy', 'track', 'estimate', 'position', 'drones', 'using', 'data', 'two', 'types', 'lidar', 'sensors', 'livox', 'avia', 'lidar', 'method', 'merges', 'data', 'lidar', 'sources', 'accurately', 'determine', 'drone', 'location', 'three', 'dimensions', 'method', 'begins', 'synchronizing', 'time', 'codes', 'data', 'two', 'sensors', 'isolates', 'point', 'cloud', 'data', 'objects', 'interest', 'oois', 'environmental', 'data', 'spatial', 'clustering', 'applications', 'noise', 'dbscan', 'method', 'applied', 'cluster', 'ooi', 'point', 'cloud', 'data', 'center', 'point', 'prominent', 'cluster', 'taken', 'drone', 'location', 'technique', 'also', 'incorporates', 'past', 'position', 'estimates', 'compensate', 'missing', 'information']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def normalize_text(tokens):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    return [word.translate(table).lower() for word in tokens if word.isalpha()]\n",
    "\n",
    "normalized_tokens = normalize_text(filtered_tokens)\n",
    "print(normalized_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['drone', 'tracking', 'and', 'localization', 'be', 'essential', 'for', 'various', 'application', 'include', 'manage', 'drone', 'formation', 'and', 'implement', 'anti', 'drone', 'strategy', 'pinpoint', 'and', 'monitor', 'drone', 'in', 'three', 'dimensional', 'space', 'be', 'difficult', 'particularly', 'when', 'try', 'to', 'capture', 'the', 'subtle', 'movement', 'of', 'small', 'drone', 'during', 'rapid', 'maneuver', 'this', 'involve', 'extract', 'faint', 'signal', 'from', 'varied', 'flight', 'setting', 'and', 'maintain', 'alignment', 'despite', 'swift', 'action', 'typically', 'camera', 'and', 'LiDAR', 'system', 'be', 'use', 'to', 'record', 'the', 'path', 'of', 'drone', 'however', 'they', 'encounter', 'challenge', 'in', 'categorize', 'drone', 'and', 'estimate', 'their', 'position', 'accurately', 'this', 'report', 'provide', 'an', 'overview', 'of', 'an', 'approach', 'name', 'CL', 'Det', 'it', 'use', 'a', 'clustering', 'base', 'learn', 'detection', 'strategy', 'to', 'track', 'and', 'estimate', 'the', 'position', 'of', 'drone', 'use', 'datum', 'from', 'two', 'type', 'of', 'lidar', 'sensor', 'Livox', 'Avia', 'and', 'LiDAR', 'this', 'method', 'merge', 'datum', 'from', 'both', 'LiDAR', 'source', 'to', 'accurately', 'determine', 'the', 'drone', 'location', 'in', 'three', 'dimension', 'the', 'method', 'begin', 'by', 'synchronize', 'the', 'time', 'code', 'of', 'the', 'datum', 'from', 'the', 'two', 'sensor', 'and', 'then', 'isolate', 'the', 'point', 'cloud', 'datum', 'for', 'the', 'object', 'of', 'interest', 'OOIs', 'from', 'the', 'environmental', 'datum', 'a', 'density', 'base', 'Spatial', 'Clustering', 'of', 'Applications', 'with', 'Noise', 'DBSCAN', 'method', 'be', 'apply', 'to', 'cluster', 'the', 'OOI', 'point', 'cloud', 'datum', 'and', 'the', 'center', 'point', 'of', 'the', 'most', 'prominent', 'cluster', 'be', 'take', 'as', 'the', 'drone', 'location', 'the', 'technique', 'also', 'incorporate', 'past', 'position', 'estimate', 'to', 'compensate', 'for', 'any', 'missing', 'information']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_tokens = [token.lemma_ for token in doc if token.is_alpha]\n",
    "print(lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = {\n",
    "    \"Abstract\": lemmatized_tokens,\n",
    "    \"Introduction\": [],  # Repeat preprocessing for other sections\n",
    "    \"Methods\": [],\n",
    "    \"Results\": [],\n",
    "    \"Conclusion\": []\n",
    "}\n",
    "\n",
    "with open('structured_data.json', 'w') as json_file:\n",
    "    json.dump(data, json_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No java install detected. Please install java to use language-tool-python.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanguage_tool_python\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LanguageTool\n\u001b[1;32m----> 3\u001b[0m tool \u001b[38;5;241m=\u001b[39m \u001b[43mLanguageTool\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men-US\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is an example sentence with errors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m matches \u001b[38;5;241m=\u001b[39m tool\u001b[38;5;241m.\u001b[39mcheck(text)\n",
      "File \u001b[1;32mc:\\Users\\jbsch\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\language_tool_python\\server.py:70\u001b[0m, in \u001b[0;36mLanguageTool.__init__\u001b[1;34m(self, language, motherTongue, remote_server, newSpellings, new_spellings_persist, host, config, language_tool_download_version)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_remote_server_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server_is_alive():\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_start_server_on_free_port\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m language \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jbsch\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\language_tool_python\\server.py:270\u001b[0m, in \u001b[0;36mLanguageTool._start_server_on_free_port\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/v2/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_port)\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 270\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_start_local_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ServerError:\n",
      "File \u001b[1;32mc:\\Users\\jbsch\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\language_tool_python\\server.py:280\u001b[0m, in \u001b[0;36mLanguageTool._start_local_server\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_start_local_server\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;66;03m# Before starting local server, download language tool if needed.\u001b[39;00m\n\u001b[1;32m--> 280\u001b[0m     \u001b[43mdownload_lt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlanguage_tool_download_version\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m     err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jbsch\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\language_tool_python\\download_lt.py:152\u001b[0m, in \u001b[0;36mdownload_lt\u001b[1;34m(language_tool_version)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_lt\u001b[39m(language_tool_version: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m LTP_DOWNLOAD_VERSION):\n\u001b[1;32m--> 152\u001b[0m     \u001b[43mconfirm_java_compatibility\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m     download_folder \u001b[38;5;241m=\u001b[39m get_language_tool_download_path()\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# Use the env var to the jar directory if it is defined\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# otherwise look in the download directory\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jbsch\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\language_tool_python\\download_lt.py:79\u001b[0m, in \u001b[0;36mconfirm_java_compatibility\u001b[1;34m()\u001b[0m\n\u001b[0;32m     77\u001b[0m java_path \u001b[38;5;241m=\u001b[39m find_executable(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjava\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m java_path:\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo java install detected. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease install java to use language-tool-python.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     82\u001b[0m     )\n\u001b[0;32m     84\u001b[0m output \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mcheck_output([java_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-version\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     85\u001b[0m                                  stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mSTDOUT,\n\u001b[0;32m     86\u001b[0m                                  universal_newlines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     88\u001b[0m major_version, minor_version \u001b[38;5;241m=\u001b[39m parse_java_version(output)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No java install detected. Please install java to use language-tool-python."
     ]
    }
   ],
   "source": [
    "from language_tool_python import LanguageTool\n",
    "\n",
    "tool = LanguageTool('en-US')\n",
    "text = \"This is an example sentence with errors.\"\n",
    "matches = tool.check(text)\n",
    "grammar_error_count = len(matches)\n",
    "print(f\"Number of grammar errors: {grammar_error_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load a pre-trained model for grammar correction\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m grammar_checker \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext2text-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprithivida/grammar_error_correcter_v1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is an example sentence with errors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m corrected \u001b[38;5;241m=\u001b[39m grammar_checker(text)\n",
      "File \u001b[1;32mc:\\Users\\jbsch\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:940\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    939\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 940\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    950\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    951\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32mc:\\Users\\jbsch\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\base.py:240\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;124;03mSelect framework (TensorFlow or PyTorch) to use from the `model` passed. Returns a tuple (framework, model).\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;124;03m    `Tuple`: A tuple framework, model.\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available():\n\u001b[1;32m--> 240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one of TensorFlow 2.0 or PyTorch should be installed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo install PyTorch, read the instructions at https://pytorch.org/.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    244\u001b[0m     )\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    246\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_pipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task\n",
      "\u001b[1;31mRuntimeError\u001b[0m: At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained model for grammar correction\n",
    "grammar_checker = pipeline(\"text2text-generation\", model=\"prithivida/grammar_error_correcter_v1\")\n",
    "\n",
    "text = \"This is an example sentence with errors.\"\n",
    "corrected = grammar_checker(text)\n",
    "print(f\"Corrected: {corrected[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
>>>>>>> 419cbddfd154b28877690b6ce9a8c98aedcb81a9
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
   "display_name": "Python 3 (ipykernel)",
=======
   "display_name": "Python 3",
>>>>>>> 419cbddfd154b28877690b6ce9a8c98aedcb81a9
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
