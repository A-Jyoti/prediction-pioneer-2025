{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1 for KDSH2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we have to categorize the research papers into publishable and non-publishable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Convert Research Papers into Structured Format\n",
    "\n",
    "The first step is to extract text and segment it into meaningful sections (like abstract, methods, results, etc.).\n",
    "a. Extract Text from PDF\n",
    "\n",
    "    Use libraries such as:\n",
    "        PyPDF2: Basic text extraction.\n",
    "        PDFMiner: Better for more complex PDFs.\n",
    "        Grobid: Specifically designed for scientific papers, extracts structured content (title, abstract, references).\n",
    "\n",
    "Example with PyPDF2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    with open(file_path, 'rb') as pdf_file:\n",
    "        reader = PyPDF2.PdfReader(pdf_file)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "        return text\n",
    "\n",
    "pdf_text = extract_text_from_pdf('research_paper.pdf')\n",
    "print(pdf_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Split Text into Sections\n",
    "\n",
    "    Pattern Matching: Use regex to identify common headings (e.g., Introduction, Methods, Results, Conclusion).\n",
    "    Tools: Grobid is excellent for automatically segmenting into structured formats.\n",
    "\n",
    "Example (Regex for Common Headings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_into_sections(text):\n",
    "    sections = re.split(r'\\b(Abstract|Introduction|Methods|Results|Discussion|Conclusion)\\b', text, flags=re.IGNORECASE)\n",
    "    return {sections[i].strip(): sections[i + 1].strip() for i in range(1, len(sections) - 1, 2)}\n",
    "\n",
    "sections = split_into_sections(pdf_text)\n",
    "print(sections['Abstract'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Tokenize and Clean Data\n",
    "\n",
    "Once the text is extracted and segmented, process it to make it suitable for analysis.\n",
    "a. Tokenization\n",
    "\n",
    "    Split text into words or sentences using tools like spaCy or NLTK.\n",
    "    For example, spaCy offers robust tokenization for different languages.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(sections['Abstract'])\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Remove Stopwords\n",
    "\n",
    "    Stopwords are common words (e.g., \"the\", \"is\", \"and\") that do not add value to the analysis.\n",
    "    Use prebuilt stopword lists from NLTK or spaCy, or customize your list.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = word_tokenize(sections['Abstract'])\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Normalize Text\n",
    "\n",
    "    Convert all text to lowercase to ensure case consistency.\n",
    "    Remove special characters, punctuation, and numbers if irrelevant.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def normalize_text(tokens):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    return [word.translate(table).lower() for word in tokens if word.isalpha()]\n",
    "\n",
    "normalized_tokens = normalize_text(filtered_tokens)\n",
    "print(normalized_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Lemmatization\n",
    "\n",
    "    Reduce words to their base or dictionary form (e.g., \"running\" → \"run\").\n",
    "    Use spaCy or WordNetLemmatizer from NLTK.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "lemmatized_tokens = [token.lemma_ for token in doc if token.is_alpha]\n",
    "print(lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Output Structured Format\n",
    "\n",
    "Store the preprocessed data in a structured format like JSON or CSV for easy access.\n",
    "\n",
    "Example JSON Format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = {\n",
    "    \"Abstract\": lemmatized_tokens,\n",
    "    \"Introduction\": [],  # Repeat preprocessing for other sections\n",
    "    \"Methods\": [],\n",
    "    \"Results\": [],\n",
    "    \"Conclusion\": []\n",
    "}\n",
    "\n",
    "with open('structured_data.json', 'w') as json_file:\n",
    "    json.dump(data, json_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional Tools\n",
    "\n",
    "    Grobid:\n",
    "        For scientific papers, Grobid provides structured extraction including metadata, references, and sections.\n",
    "        Grobid Documentation\n",
    "\n",
    "    Other Libraries:\n",
    "        Textract: Handles different file types (PDF, DOCX, etc.).\n",
    "        Tika: Extracts text and metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Language Quality\n",
    "\n",
    "Assess grammar, coherence, and readability.\n",
    "a. Grammar and Spelling\n",
    "\n",
    "    Tools:\n",
    "        LanguageTool: An open-source grammar and spell checker.\n",
    "        Grammarly API: (Paid) for advanced grammar checking.\n",
    "\n",
    "    Implementation: Use LanguageTool to calculate the number of grammatical errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from language_tool_python import LanguageTool\n",
    "\n",
    "tool = LanguageTool('en-US')\n",
    "text = \"This is an example sentence with errors.\"\n",
    "matches = tool.check(text)\n",
    "grammar_error_count = len(matches)\n",
    "print(f\"Number of grammar errors: {grammar_error_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Coherence\n",
    "\n",
    "    Measure how logically connected the sentences are.\n",
    "    Semantic Similarity:\n",
    "        Use embeddings (e.g., SBERT or GPT embeddings) to calculate the cosine similarity between consecutive sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "sentences = [\"This is the first sentence.\", \"This is the second sentence.\"]\n",
    "embeddings = model.encode(sentences)\n",
    "coherence_score = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "print(f\"Coherence score: {coherence_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Readability\n",
    "\n",
    "    TextStat: Calculate readability scores like Flesch Reading Ease, Gunning Fog Index, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import textstat\n",
    "\n",
    "text = \"This is a sample sentence to calculate readability.\"\n",
    "flesch_score = textstat.flesch_reading_ease(text)\n",
    "print(f\"Flesch Reading Ease score: {flesch_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Methodology Validity\n",
    "\n",
    "Assess whether the methods are appropriate for the research objectives.\n",
    "a. Keyword Analysis\n",
    "\n",
    "    Extract keywords from the \"Methods\" section and compare them with the \"Objectives\" section.\n",
    "    Use KeyBERT or TF-IDF for keyword extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "kw_model = KeyBERT()\n",
    "methods_text = \"We used neural networks to classify images.\"\n",
    "keywords = kw_model.extract_keywords(methods_text, keyphrase_ngram_range=(1, 2), stop_words='english')\n",
    "print(keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Concept Matching\n",
    "\n",
    "    Use embeddings to check if the topics covered in the methodology align with the research objectives.\n",
    "        Generate embeddings for both sections and compute cosine similarity.\n",
    "\n",
    "c. Completeness of Methodology\n",
    "\n",
    "    Check for the presence of essential subsections:\n",
    "        Tools/technologies used.\n",
    "        Dataset description.\n",
    "        Experimental setup.\n",
    "        Statistical tests or validation techniques.\n",
    "    Example: Use regex to search for keywords like \"dataset,\" \"tool,\" or \"experiment.\"\n",
    "\n",
    "3. Claim Validation\n",
    "\n",
    "Identify overly ambitious or unsupported claims.\n",
    "a. Fact-Checking Claims\n",
    "\n",
    "    Tools:\n",
    "        Use pre-trained models like TARS-QA (from Hugging Face) or fine-tune BERT for fact-checking tasks.\n",
    "    Approach:\n",
    "        Extract claims from the text (e.g., \"This method improves accuracy by 20%\").\n",
    "        Compare claims with the supporting data in the results section.\n",
    "\n",
    "b. Hyperbolic Language Detection\n",
    "\n",
    "    Look for hyperbolic phrases using pattern matching or pre-trained models.\n",
    "        Example: Check for words like \"revolutionary,\" \"breakthrough,\" \"unprecedented,\" etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = \"Our method achieves unprecedented accuracy.\"\n",
    "hyperbolic_words = [\"revolutionary\", \"breakthrough\", \"unprecedented\"]\n",
    "found_hyperbolic = [word for word in hyperbolic_words if word in text.lower()]\n",
    "print(f\"Hyperbolic words: {found_hyperbolic}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Claim-Data Alignment\n",
    "\n",
    "    Calculate alignment between claims and data:\n",
    "        Compare numerical claims with reported experimental results.\n",
    "        Check consistency in units, scales, and statistical validity.\n",
    "\n",
    "4. Additional Features\n",
    "\n",
    "Beyond the specified aspects, consider these additional features:\n",
    "a. Structural Features\n",
    "\n",
    "    Section Lengths:\n",
    "        Measure the word count of each section (e.g., abstract, methods) to detect imbalance or missing sections.\n",
    "    Reference Count:\n",
    "        Count the number of references cited in the paper.\n",
    "\n",
    "b. Semantic Features\n",
    "\n",
    "    Use Topic Modeling (e.g., LDA) to identify dominant topics in the paper.\n",
    "    Sentiment Analysis:\n",
    "        Analyze sentiment in the discussion or conclusion sections to detect optimism or bias.\n",
    "\n",
    "c. Statistical Features\n",
    "\n",
    "    Calculate the proportion of cited references to sentences to measure scientific rigor.\n",
    "\n",
    "5. Feature Storage\n",
    "\n",
    "Store extracted features in a structured format like a Pandas DataFrame or JSON for further analysis.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "features = {\n",
    "    \"grammar_errors\": grammar_error_count,\n",
    "    \"coherence_score\": coherence_score,\n",
    "    \"flesch_score\": flesch_score,\n",
    "    \"method_objective_alignment\": 0.85,  # Example similarity score\n",
    "    \"hyperbolic_words_count\": len(found_hyperbolic)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame([features])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Tools and Libraries\n",
    "\n",
    "    Language Quality:\n",
    "        spaCy, TextStat, Sentence Transformers, LanguageTool.\n",
    "    Methodology Validity:\n",
    "        KeyBERT, Regex, Hugging Face models.\n",
    "    Claim Validation:\n",
    "        Hugging Face Transformers, regex.\n",
    "    Storage:\n",
    "        Pandas, NumPy.\n",
    "\n",
    "This process ensures comprehensive feature extraction, providing critical insights for publishability classification. Let me know if you'd like help with specific implementations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "1. Challenges and Considerations\n",
    "\n",
    "    Small Dataset: Only 15 labeled reference papers are provided, so data scarcity is a significant challenge.\n",
    "    Complexity: Research papers contain dense information, requiring models that can handle long-form text.\n",
    "    Solution: Use transfer learning with pre-trained models like BERT or similar transformers to leverage knowledge from large datasets.\n",
    "\n",
    "2. Pipeline for Model Training\n",
    "a. Preprocessing the Dataset\n",
    "\n",
    "    Text Cleaning:\n",
    "        Apply preprocessing steps (as described earlier): tokenization, stopword removal, lemmatization, etc.\n",
    "        Ensure that each paper's sections (abstract, methods, results) are merged into a single input text or treated as separate features.\n",
    "\n",
    "    Label Encoding:\n",
    "        Encode labels into binary values:\n",
    "            Publishable → 1\n",
    "            Non-Publishable → 0\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"text\": \"Paper 1 content...\", \"label\": 1},\n",
    "    {\"text\": \"Paper 2 content...\", \"label\": 0},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Train-Test Split:\n",
    "        Use an 80-20 split for training and testing.\n",
    "        Optionally, use k-fold cross-validation (e.g., 5 folds) for better model reliability.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "texts = [d[\"text\"] for d in data]\n",
    "labels = [d[\"label\"] for d in data]\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Choose a Pre-trained Transformer Model\n",
    "\n",
    "    Use a pre-trained transformer model from Hugging Face:\n",
    "        BERT: General-purpose transformer for language tasks.\n",
    "        SciBERT: Specifically trained on scientific literature.\n",
    "        DistilBERT: Lightweight version of BERT for faster inference.\n",
    "\n",
    "Install Hugging Face Transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pip install transformers datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Tokenization\n",
    "\n",
    "    Use the tokenizer corresponding to the chosen pre-trained model.\n",
    "    Tokenize input text, truncating or padding to handle variable lengths.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"allenai/scibert_scivocab_uncased\"  # Replace with \"bert-base-uncased\" if not using SciBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Model Fine-Tuning\n",
    "\n",
    "    Load Pre-trained Model:\n",
    "        Load a pre-trained transformer model with a classification head (e.g., BERTForSequenceClassification).\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Binary classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Define the Optimizer and Scheduler:\n",
    "        Use AdamW optimizer and a learning rate scheduler for efficient training.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Define Loss Function:\n",
    "        Use Cross-Entropy Loss for binary classification tasks.\n",
    "\n",
    "    Training Loop:\n",
    "        Use Hugging Face’s Trainer API or manually define a PyTorch training loop.\n",
    "\n",
    "Example (Using Trainer API):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# Convert labels to tensors\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset={\n",
    "        \"input_ids\": train_encodings[\"input_ids\"],\n",
    "        \"attention_mask\": train_encodings[\"attention_mask\"],\n",
    "        \"labels\": train_labels,\n",
    "    },\n",
    "    eval_dataset={\n",
    "        \"input_ids\": val_encodings[\"input_ids\"],\n",
    "        \"attention_mask\": val_encodings[\"attention_mask\"],\n",
    "        \"labels\": val_labels,\n",
    "    },\n",
    "    optimizers=(optimizer, None),  # No scheduler for simplicity\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Evaluate the Model\n",
    "\n",
    "    Metrics:\n",
    "        Accuracy, Precision, Recall, F1-Score.\n",
    "        Use sklearn for metric calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predictions = trainer.predict(val_encodings)\n",
    "preds = predictions.predictions.argmax(-1)\n",
    "print(classification_report(val_labels, preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Confusion Matrix:\n",
    "        Visualize performance using a confusion matrix.\n",
    "\n",
    "4. Handle Small Dataset Challenges\n",
    "a. Data Augmentation\n",
    "\n",
    "    Back-Translation: Translate text to another language and back to introduce variations.\n",
    "    Paraphrasing: Use models like T5 or Pegasus to rephrase sentences.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "paraphraser = pipeline(\"text2text-generation\", model=\"t5-small\")\n",
    "paraphrased_text = paraphraser(\"Your original text here\", max_length=50, num_return_sequences=1)\n",
    "print(paraphrased_text[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Transfer Learning\n",
    "\n",
    "    Fine-tune the pre-trained model on domain-specific datasets (e.g., research papers) before training on the 15 labeled examples.\n",
    "\n",
    "c. Use Few-Shot Learning\n",
    "\n",
    "    Leverage prompt engineering with large models (e.g., OpenAI GPT-3/4) for few-shot classification:\n",
    "        Provide labeled examples as part of the input prompt.\n",
    "\n",
    "5. Save and Deploy the Model\n",
    "\n",
    "    Save Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./saved_model\")\n",
    "tokenizer.save_pretrained(\"./saved_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Deploy Model:\n",
    "        Use FastAPI or Flask to serve the model as an API for real-time predictions.\n",
    "\n",
    "This process provides a robust framework to fine-tune a pre-trained model for the task, even with a small labeled dataset. Let me know if you'd like help implementing a specific part!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating your model is a crucial step in understanding its performance and ensuring it generalizes well to unseen data. Here's a detailed guide to evaluating a classification model using Accuracy, F1-Score, and Confusion Matrix:\n",
    "1. Dataset Setup\n",
    "\n",
    "Ensure that you have a validation dataset (or test dataset) that was not used during training.\n",
    "\n",
    "    Input: Text data and corresponding true labels (val_texts and val_labels).\n",
    "    Output: Predicted labels from the model and metrics to evaluate the predictions.\n",
    "\n",
    "2. Perform Model Predictions\n",
    "\n",
    "    Prepare Validation Data:\n",
    "        Tokenize the validation data if not already done.\n",
    "\n",
    "    val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "    Get Predictions:\n",
    "        Use the trained model to predict on the validation dataset.\n",
    "        Extract raw logits (output scores) and convert them into class predictions.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Perform prediction\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        input_ids=val_encodings[\"input_ids\"],\n",
    "        attention_mask=val_encodings[\"attention_mask\"]\n",
    "    )\n",
    "logits = outputs.logits\n",
    "predicted_labels = torch.argmax(logits, dim=1).cpu().numpy()  # Get class predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True Labels: Ensure your true labels (val_labels) are stored in the same format as predictions (e.g., as a NumPy array):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "true_labels = val_labels.numpy() if isinstance(val_labels, torch.Tensor) else val_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Metrics Overview\n",
    "a. Accuracy\n",
    "\n",
    "    Definition: The percentage of correctly classified samples.\n",
    "    Formula:\n",
    "    Accuracy=Number of Correct PredictionsTotal Number of Predictions\n",
    "    Accuracy=Total Number of PredictionsNumber of Correct Predictions​\n",
    "\n",
    "b. F1-Score\n",
    "\n",
    "    Definition: Harmonic mean of Precision and Recall, especially useful when the dataset is imbalanced.\n",
    "    Formula:\n",
    "    F1-Score=2⋅Precision⋅RecallPrecision+Recall\n",
    "    F1-Score=2⋅Precision+RecallPrecision⋅Recall​\n",
    "    Where:\n",
    "        Precision: Percentage of correctly predicted positive samples.\n",
    "        Precision=True PositivesTrue Positives+False Positives\n",
    "        Precision=True Positives+False PositivesTrue Positives​\n",
    "        Recall: Percentage of actual positives correctly predicted.\n",
    "        Recall=True PositivesTrue Positives+False Negatives\n",
    "        Recall=True Positives+False NegativesTrue Positives​\n",
    "\n",
    "c. Confusion Matrix\n",
    "\n",
    "    Definition: A matrix that summarizes the counts of true positive, true negative, false positive, and false negative predictions.\n",
    "    Format:\n",
    "    \tPredicted Positive\tPredicted Negative\n",
    "    Actual Positive\tTrue Positive (TP)\tFalse Negative (FN)\n",
    "    Actual Negative\tFalse Positive (FP)\tTrue Negative (TN)\n",
    "\n",
    "4. Compute Metrics\n",
    "\n",
    "    Using scikit-learn: Install scikit-learn if not already installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Accuracy, Precision, Recall, F1-Score, and Confusion Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "# F1-Score\n",
    "f1 = f1_score(true_labels, predicted_labels, average='binary')  # Use 'macro' or 'weighted' for multi-class\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Classification Report (Precision, Recall, F1-Score for each class)\n",
    "report = classification_report(true_labels, predicted_labels)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1-Score: {f1}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Confusion Matrix:\n",
    "\n",
    "    Use matplotlib or seaborn to plot the confusion matrix for better interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Non-Publishable\", \"Publishable\"], yticklabels=[\"Non-Publishable\", \"Publishable\"])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Example Output\n",
    "\n",
    "Given the following sample results:\n",
    "\n",
    "    True Labels: [1, 0, 1, 1, 0]\n",
    "    Predicted Labels: [1, 0, 1, 0, 0]\n",
    "\n",
    "The metrics would look like:\n",
    "\n",
    "    Accuracy: 45=0.854​=0.8 (80%)\n",
    "    F1-Score: 0.80.8\n",
    "    Confusion Matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Considerations\n",
    "\n",
    "    Imbalanced Datasets:\n",
    "        If one class dominates (e.g., most papers are non-publishable), focus more on F1-Score than Accuracy, as Accuracy can be misleading in such cases.\n",
    "    Threshold Adjustments:\n",
    "        If logits are used for predictions, adjust the decision threshold to optimize precision or recall based on the use case.\n",
    "\n",
    "This process ensures a comprehensive evaluation of your model’s performance, highlighting strengths and areas for improvement. Let me know if you need help implementing these steps!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Output\n",
    "\n",
    "    Create a binary output: 1 (Publishable) or 0 (Non-Publishable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Batch Prediction for Multiple Papers\n",
    "\n",
    "If multiple research papers need to be classified, process them in batches.\n",
    "Steps:\n",
    "\n",
    "    Tokenize all papers in a batch using the tokenizer.\n",
    "    Pass the batch through the model for inference.\n",
    "    Collect and store predictions for each paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "batch_texts = [\n",
    "    \"Paper 1 content...\",\n",
    "    \"Paper 2 content...\",\n",
    "    \"Paper 3 content...\"\n",
    "]\n",
    "\n",
    "# Tokenize the batch\n",
    "batch_encodings = tokenizer(batch_texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# Predict for the batch\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        input_ids=batch_encodings[\"input_ids\"],\n",
    "        attention_mask=batch_encodings[\"attention_mask\"]\n",
    "    )\n",
    "\n",
    "# Convert logits to probabilities and get predictions\n",
    "logits = outputs.logits\n",
    "probs = F.softmax(logits, dim=1)\n",
    "predictions = torch.argmax(probs, dim=1).tolist()\n",
    "\n",
    "# Combine results with confidence scores\n",
    "output_data = [\n",
    "    {\"Paper ID\": idx + 1, \"Prediction\": pred, \"Confidence\": probs[i][pred].item()}\n",
    "    for i, (idx, pred) in enumerate(zip(range(len(batch_texts)), predictions))\n",
    "]\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(output_data)\n",
    "df.to_csv(\"batch_predictions.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
